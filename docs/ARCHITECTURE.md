# System Architecture Documentation

## 1. Introduction/Overview

This document outlines the architecture of the PDF Assistant Chatbot. The system comprises a FastAPI-based Python backend and a Streamlit-based Python frontend. The backend is designed to process PDF documents, store their content in a vector database (Pinecone), and answer user questions using a Retrieval Augmented Generation (RAG) approach with Google Gemini. The frontend provides a user interface for uploading PDFs and interacting with the chatbot.

The architecture emphasizes modularity, separating concerns into distinct components for the user interface, backend request handling, core processing logic, external service interactions, and configuration management. This approach aims for maintainability, scalability, and clarity.

## 2. Core Components

The application is structured into several key components:

### 2.1. Frontend (Streamlit Application - `src/frontend/streamlit_app.py`)

-   **`src/frontend/streamlit_app.py`**: This is the main application file for the user interface, built using Streamlit.
-   **Key Responsibilities**:
    -   Provides a user-friendly interface for uploading PDF documents.
    -   Allows users to input questions in a chat-like interface.
    -   Communicates with the FastAPI backend API (via HTTP requests) to:
        -   Send PDF files for processing.
        -   Submit user queries and display the answers received from the backend.
    -   Manages UI state and user interactions.
    -   Requires the `ENDPOINT` environment variable to be set to the URL of the backend API.

### 2.2. Backend FastAPI Application (`app.py`, `src/backend/__init__.py`)

-   **`app.py`**: This is the main executable script at the project root. It imports the `create_app` factory function from `src.backend` and uses it to get a FastAPI application instance. It then runs this app instance using an ASGI server like Uvicorn, handling settings like host, port, and reload based on the application's configuration (derived from `Config`).
-   **`src/backend/__init__.py`**: This file contains the `create_app()` factory function. This function is responsible for:
    -   Creating the main `FastAPI` application instance.
    -   Loading application configurations from `src.backend.config.Config` and making it available in the app's state (e.g., `app.state.config`).
    -   Initializing the `ChatbotAgent` (an instance of `RAGAgent` from `src.backend.agents.rag_agent.py`) and making it available, for example, via `app.state.chatbot_agent`. This agent encapsulates the core RAG logic.
    -   Initializing an `Orchestrator` (from `src.backend.services.orchestrator.py`) with the `ChatbotAgent` instance. The orchestrator can also be stored in `app.state` if needed by routes directly, or the agent can be injected into routes.
    -   Including API Routers. For example, it imports `chat_router` from `src.backend.routes.chat` and includes it in the FastAPI app, often with a prefix (e.g., `/api/chat`).
    -   Configuring application-level logging.
    -   Setting up middleware, such as `CORSMiddleware` for handling Cross-Origin Resource Sharing.

### 2.3. Backend Routing (`src/backend/routes/chat.py`)

-   API endpoints are modularized using FastAPI's `APIRouter`. The primary router for chatbot functionalities, `chat_router`, is defined in this module.
-   This module is responsible for defining path operations for various chat-related functionalities using decorators like `@chat_router.get()` or `@chat_router.post()`:
    -   `GET /`: Provides basic information about the chat API.
    -   `GET /health`: Performs a health check of the `ChatbotAgent` and its dependencies.
    -   `POST /uploadpdf`: Handles PDF file uploads. It uses FastAPI's `UploadFile` for efficient file handling and `Depends` for injecting the `ChatbotAgent` and `Config` instances. It validates file types and sizes.
    -   `POST /answer`: Receives user queries (validated by the `AnswerRequest` Pydantic model) and returns answers generated by the agent.
-   Path operation functions utilize FastAPI's dependency injection system (e.g., `agent: ChatbotAgent = Depends(get_chatbot_agent)`) to access shared application components like the `ChatbotAgent` instance (initialized in `src/backend/__init__.py` and stored in app state) and the application `Config`.
-   Pydantic models (e.g., `AnswerRequest`, `HealthResponse`, `UploadResponse`, `AnswerResponse`, `ErrorResponse`) are used for automatic request data validation, response data serialization, and generating OpenAPI schema for documentation.
-   Core business logic is delegated to methods of the injected `ChatbotAgent` instance.
-   FastAPI automatically handles JSON response formatting and setting appropriate HTTP status codes, including handling `HTTPException` for errors.

### 2.4. Backend Agent (`src/backend/agents/rag_agent.py` - Class `ChatbotAgent`)

-   The `ChatbotAgent` class, which is an implementation of the `BaseChatbotAgent` abstract class (defined in `src/backend/agents/base.py`), encapsulates the core RAG (Retrieval Augmented Generation) logic.
-   **Key Responsibilities**:
    -   **Initialization**:
        -   Establishes connections with external services: Google Gemini (for both Large Language Model capabilities and text embeddings) and Pinecone (for the vector database).
        -   Uses API keys and configuration parameters loaded from the `Config` class.
        -   Leverages Langchain library components for streamlined interaction: `GoogleGenerativeAIEmbeddings` for creating vector representations of text and `PineconeVectorStore` for interfacing with the Pinecone index.
    -   **PDF Processing (`upload_data` method)**:
        -   Accepts a PDF file path.
        -   Uses `PyPDFLoader` (from Langchain) to load and parse text content from the PDF.
        -   Employs `RecursiveCharacterTextSplitter` (from Langchain) to divide the extracted text into smaller, semantically coherent chunks suitable for embedding.
        -   Adds relevant metadata (e.g., `document_type`, `userId` for resumes) to each text chunk.
        -   Generates vector embeddings for each chunk using the configured Gemini embedding model (e.g., `models/embedding-001`).
        -   Stores these text chunks and their corresponding vector embeddings in the Pinecone vector index, making them searchable.
    -   **Retrieval Augmented Generation (RAG) for Question Answering (`answer_question` method)**:
        -   Receives a user's query.
        -   Generates an embedding for the query using the same Gemini embedding model.
        -   Performs a similarity search against the Pinecone vector index to retrieve the most relevant text chunks (context) based on vector proximity to the query embedding.
        -   Constructs a detailed prompt for the Gemini LLM (e.g., `gemini-2.0-flash`) using a predefined template. This prompt includes the retrieved context and the original user question, guiding the LLM to answer based on the provided information.
        -   Sends the prompt to the LLM and receives the generated natural language answer.
    -   **Health Checks (`health_check` method)**: Provides functionality to verify the operational status of its dependencies (Gemini API, Pinecone connection, embedding model, and vector store).

### 2.5. Backend Services (`src/backend/services/orchestrator.py` - Class `Orchestrator`)

-   The `Orchestrator` class is designed to act as an intermediary or service layer. In the current setup, it is initialized with an instance of `ChatbotAgent`.
-   **Key Responsibilities**:
    -   It holds an instance of the `ChatbotAgent`.
    -   It exposes methods like `process_query(query: str)`, `ingest_document(file_path: str, user_id: str = None)`, and `get_service_health()`.
    -   These methods directly delegate their operations to the corresponding methods of the `ChatbotAgent` instance (e.g., `self.chatbot_agent.answer_question(query)`).
-   **Usage in Application**:
    -   An instance of the `Orchestrator` is created in `src/backend/__init__.py` during app setup (e.g., `orchestrator = Orchestrator(chatbot_agent=chatbot_agent)`).
    -   However, the FastAPI route handlers in `src/backend/routes/chat.py` primarily obtain the `ChatbotAgent` instance directly via dependency injection (`Depends(get_chatbot_agent)` which retrieves it from `request.app.state.chatbot_agent`).
    -   Thus, while the `Orchestrator` is present and functional, the current routing logic in `chat.py` bypasses it for direct agent interaction. If the system were to evolve to include more complex interactions or multiple agents, the `Orchestrator` could become the primary interface for the route handlers.
-   **Benefits & Extensibility (if used as the primary interface by routes)**:
    -   Decouples API route logic from the specific agent's implementation details.
    -   Provides a dedicated layer for implementing more sophisticated logic, such as query routing to different specialized agents, combining results from multiple sources, or managing conversational context across multiple turns, without cluttering the route handlers or the agent itself.

### 2.6. Backend Configuration (`src/backend/config.py` - Class `Config`)

-   The `Config` class is the central point for managing all application settings for the backend.
-   **Key Functions**:
    -   Uses `dotenv.load_dotenv()` to load environment variables from a `.env` file at the project root (if one exists). This allows for easy configuration during local development without hardcoding secrets.
    -   Retrieves configuration values using `os.getenv()`, providing sensible default values for many settings (e.g., `PINECONE_CLOUD` defaults to "aws", `PORT` defaults to 5000).
    -   Defines and stores various application-wide settings as class attributes:
        -   **File Handling**: `MAX_FILE_SIZE` (e.g., 50MB), `ALLOWED_EXTENSIONS` (e.g., `{'pdf'}`), `UPLOAD_FOLDER` (e.g., 'uploads', and ensures this directory is created).
        -   **External Service Credentials & Settings**: `GEMINI_API_KEY`, `PINECONE_API_KEY`, `PINECONE_INDEX_NAME`, `PINECONE_CLOUD`, `PINECONE_REGION`.
        -   **Server Settings**: `HOST` (e.g., '0.0.0.0'), `PORT` (e.g., 5000), `DEBUG` (boolean, derived from environment variable).
    -   Includes a static method `validate_required_env_vars()` which checks for the presence of essential environment variables (`GEMINI_API_KEY`, `PINECONE_API_KEY`, `PINECONE_INDEX_NAME`). This method is called by the `ChatbotAgent` during its initialization to ensure critical configurations are available, raising a `ValueError` if any are missing.

### 2.7. Backend Utilities (`src/backend/utils/helper.py`)

-   This module (`src/backend/utils/helper.py`) is currently present in the project structure.
-   Based on the provided file contents, this file is empty or does not contain any active utility functions that are explicitly imported or used by the core components reviewed (agent, routes, config, orchestrator).
-   **Intended Purpose**: It serves as a designated location for common, reusable utility functions or helper classes that might be beneficial across various parts of the backend. Examples could include specialized text processing functions not covered by Langchain, custom data validation routines, date/time manipulation helpers, or other miscellaneous logic that doesn't fit neatly into other specific components.

## 3. Directory Structure Overview

The project's codebase (assuming root folder name `PDF-Assistant-Chatbot` or similar) is organized as follows:

```plaintext
PDF-Assistant-Chatbot/
├── .env                           # Local environment variables (gitignored)
├── .env.template                  # Template for .env file
├── .github/                       # GitHub specific files (workflows, issue templates)
├── .gitignore                     # Specifies intentionally untracked files for Git
├── CODE_OF_CONDUCT.md             # Code of conduct for contributors
├── CONTRIBUTING.md                # Guidelines for contributing to the project
├── Makefile                       # Defines common tasks (install, test, lint, run)
├── README.md                      # Main project documentation file
├── app.py                         # Main FastAPI application entry point (runs Uvicorn)
├── docs/                          # Documentation files
│   ├── API.md                     # (Potentially outdated or needs review)
│   ├── ARCHITECTURE.md            # This document
│   ├── DEPLOYMENT.md              # (Potentially needs review for FastAPI specifics)
│   ├── INSTALLATION.md            # (Potentially needs review for FastAPI specifics)
├── requirements-dev.txt           # Development-specific Python dependencies (pytest, linters)
├── requirements.txt               # Python package dependencies for the application
├── scripts/                       # Utility scripts (e.g., start.sh)
│   └── start.sh                   # Shell script for starting the backend (Uvicorn + Gunicorn)
├── src/                           # Main source code directory
│   ├── backend/                   # Source code for the FastAPI backend
│   │   ├── __init__.py            # Backend package initializer, contains create_app()
│   │   ├── agents/                # Houses different agent implementations
│   │   │   ├── __init__.py         # Agents package initializer
│   │   │   ├── base.py            # Defines BaseChatbotAgent abstract class
│   │   │   └── rag_agent.py       # Implements ChatbotAgent (RAG logic)
│   │   ├── config.py              # Centralized backend application configuration (Config class)
│   │   ├── routes/                # Defines API endpoints using FastAPI APIRouters
│   │   │   ├── __init__.py         # Routes package initializer
│   │   │   └── chat.py            # Chat-related API endpoint definitions (chat_router)
│   │   ├── services/              # Service layer components
│   │   │   ├── __init__.py         # Services package initializer
│   │   │   └── orchestrator.py     # Orchestrator class
│   │   └── utils/                 # Backend utility functions and helpers
│   │       ├── __init__.py         # Utils package initializer
│   │       └── helper.py            # Miscellaneous helper functions (currently empty)
│   └── frontend/                  # Source code for the Streamlit frontend
│       └── streamlit_app.py         # Main Streamlit application file
└── tests/                         # Directory for automated tests
    ├── conftest.py                # Pytest configuration and fixtures
    ├── test_agents/               # Tests for agent logic
    │   └── test_rag_agent.py
    └── test_routes/               # Tests for API routes
        └── test_chat_routes.py
```

This structure promotes a clear separation of concerns between the frontend (Streamlit UI) and backend (FastAPI API), and further modularizes the backend into configuration, agents, routes, services, and utilities.

## 4. Data Flow / Process Flow

The primary interactions involve the user (via the Streamlit frontend) and the FastAPI backend. The backend API endpoints are prefixed (e.g. `/api/chat`).

### 4.1. PDF Upload and Processing Flow

1.  **User Action (Frontend)**: User selects a PDF file and clicks "Upload" in the Streamlit UI (`streamlit_app.py`).
2.  **Frontend Request**: The Streamlit app (`APIClient.upload_pdf`) sends a `POST` request to the backend's `/api/chat/uploadpdf` endpoint (URL constructed from `ENDPOINT` env var), with the PDF file in `multipart/form-data`.
3.  **Backend Routing (`src/backend/routes/chat.py`)**: The `/api/chat/uploadpdf` path operation function in the FastAPI backend:
    -   Receives the request. FastAPI's `UploadFile` type is used for the file parameter, allowing efficient handling.
    -   The `ChatbotAgent` and `Config` instances are injected via FastAPI's dependency injection (`Depends`).
    -   Validates the file:
        -   Checks if `file.filename` exists.
        -   Uses `allowed_file(file.filename, config)` to check the extension against `config.ALLOWED_EXTENSIONS`.
        -   Checks `file.size` against `config.MAX_FILE_SIZE`.
    -   Temporarily saves the valid PDF file to the server's filesystem using `tempfile.NamedTemporaryFile`.
4.  **Backend Agent Call**:
    -   The path operation function directly calls `agent.upload_data(temp_file_path, user_id)`, where `agent` is the injected `ChatbotAgent` instance. `user_id` is derived from the filename if it contains 'resume' or 'cv'.
    -   (The `Orchestrator.ingest_document()` method exists but is not directly used by this route in the current implementation; the route interacts directly with the agent.)
5.  **Backend Agent Processing (`src/backend/agents/rag_agent.py` - `upload_data` method of `ChatbotAgent`)**:
    -   **Parse PDF**: Loads text from the PDF at `temp_file_path` using `PyPDFLoader`.
    -   **Chunk Text**: Splits the extracted text into smaller, manageable chunks using `RecursiveCharacterTextSplitter`.
    -   **Add Metadata**: Assigns `document_type` ('event_document' or 'resume') and `userId` (if provided) to the metadata of each chunk.
    -   **Generate Embeddings**: For each text chunk, creates a vector embedding using `GoogleGenerativeAIEmbeddings` (configured with `Config.GEMINI_API_KEY`).
    -   **Store in Vector DB**: Upserts (adds or updates) the text chunks and their corresponding vector embeddings into the Pinecone index using `PineconeVectorStore`.
6.  **Backend HTTP Response**:
    -   The `agent.upload_data` method returns `True` on success, `False` on failure.
    -   The route handler constructs a JSON response using the `UploadResponse` Pydantic model (e.g., `{"success": true, "message": "...", "filename": "..."}`) or raises an `HTTPException` if processing fails or validation checks (like file type/size) fail.
    -   The temporary file is deleted in a `finally` block.
7.  **Frontend Update**: The Streamlit app (`APIClient.upload_pdf` and `PDFUploader._handle_pdf_upload`) receives the response and updates the UI to inform the user of the upload status (e.g., success message, error message).

**Text-based Diagram (Upload Flow):**
```
User --(Uploads PDF)--> [Streamlit Frontend: PDFUploader & APIClient]
  | (POST /api/chat/uploadpdf + PDF File)
  v
[FastAPI Backend: /api/chat/uploadpdf in routes/chat.py]
  | Dependencies: ChatbotAgent, Config
  | Action: Validate file, save to temp_file_path
  | (temp_file_path, user_id?)
  v
[ChatbotAgent: upload_data()]
  | 1. Load PDF (PyPDFLoader)
  | 2. Split Text (RecursiveCharacterTextSplitter)
  | 3. Add Metadata
  | 4. Generate Embeddings (GoogleGenerativeAIEmbeddings)
  | 5. Store in Pinecone (PineconeVectorStore)
  | (Success/Failure Boolean)
  v
[FastAPI Backend: /api/chat/uploadpdf] --(JSON Response: UploadResponse / ErrorResponse)--> [Streamlit Frontend] --(UI Update)--> User
```

### 4.2. Question Answering Flow

1.  **User Action (Frontend)**: User types a question into the chat input in the Streamlit UI (`ChatUI.render_chat_interface`) and submits it.
2.  **Frontend Request**: The Streamlit app (`APIClient.send_query`) sends a `POST` request to the backend's `/api/chat/answer` endpoint (URL from `ENDPOINT` env var) with a JSON payload: `{"query": "User's question"}` (matching `AnswerRequest` Pydantic model).
3.  **Backend Routing (`src/backend/routes/chat.py`)**: The `/api/chat/answer` path operation function:
    -   Receives the request. FastAPI validates the JSON payload against the `AnswerRequest` Pydantic model (ensuring `query` is a non-empty string).
    -   The `ChatbotAgent` instance is injected via `Depends(get_chatbot_agent)`.
4.  **Backend Agent Call**:
    -   The path operation function calls `agent.answer_question(request_data.query)`, where `agent` is the injected `ChatbotAgent`.
    -   (The `Orchestrator.process_query()` method exists but is not directly used by this route; interaction is direct with the agent.)
5.  **Backend Agent Processing (`src/backend/agents/rag_agent.py` - `answer_question` method of `ChatbotAgent`)**:
    -   **Retrieve Context**: Performs a similarity search using `self.vectorstore.similarity_search_with_score(question, k=top_k)` to find relevant text chunks from Pinecone based on the user's `question`.
    -   **Format Context**: Concatenates the page content of retrieved documents to form `context_text`.
    -   **Prompt Engineering**: Creates a prompt string using `ChatPromptTemplate.from_template(self.prompt_template).format(context=context_text, question=question)`. The template instructs the LLM ("Event Bot") on how to behave and use the provided context.
    -   **Generate Answer**: Sends the complete prompt to the Gemini LLM (`self.llm.generate_content(prompt)`).
    -   The method returns a dictionary containing the `answer`, `context_found` status, `num_sources`, `success` status, and any `error`.
6.  **Backend HTTP Response**:
    -   The route handler takes the `answer` field from the dictionary returned by the agent.
    -   It sends a JSON response structured according to the `AnswerResponse` Pydantic model (e.g., `{"answer": "AI generated text."}`).
    -   If an exception occurs (e.g., agent processing error, or if the query was invalid initially), an `HTTPException` is raised, leading to an appropriate JSON error response.
7.  **Frontend Update**: The Streamlit app (`APIClient.send_query` and `ChatUI._handle_user_input`) receives the JSON response, extracts the answer, and displays it in the chat interface.

**Text-based Diagram (Question Answering Flow):**
```
User --(Asks Question)--> [Streamlit Frontend: ChatUI & APIClient]
  | (POST /api/chat/answer + JSON: {"query": "..."})
  v
[FastAPI Backend: /api/chat/answer in routes/chat.py]
  | Dependencies: ChatbotAgent
  | Request Body: AnswerRequest (validated by Pydantic)
  | (query string)
  v
[ChatbotAgent: answer_question()]
  | 1. Similarity Search (PineconeVectorStore) -> Context
  | 2. Format Context
  | 3. Build Prompt (ChatPromptTemplate)
  | 4. Generate Answer (Gemini LLM)
  | (Dictionary with "answer", "success", etc.)
  v
[FastAPI Backend: /api/chat/answer] --(JSON Response: AnswerResponse / ErrorResponse)--> [Streamlit Frontend] --(Display Answer)--> User
```

## 5. Configuration Management

Configuration is primarily managed via environment variables, facilitated by `.env` files for local development, and loaded by the `Config` class in the backend.

-   **Backend Configuration (`src/backend/config.py` - Class `Config`)**:
    -   As detailed in section 2.6, this class loads all backend-specific settings (API keys, Pinecone details, server settings like `DEBUG`, `HOST`, `PORT`, file handling parameters) from environment variables using `python-dotenv` and `os.getenv`.
    -   It provides defaults and includes validation for essential variables.
    -   This `Config` object is instantiated and made available to the FastAPI application via `app.state.config` in `src/backend/__init__.py`, and can be accessed by dependencies (like route handlers) through `request.app.state.config` or a dedicated dependency function.
-   **Frontend Configuration (`src/frontend/streamlit_app.py`)**:
    -   The Streamlit application (`APIClient` class) primarily requires the `ENDPOINT` environment variable. This variable should contain the full base URL of the backend's chat API (e.g., `http://localhost:8000/api/chat` or `http://localhost:5000/api/chat`, depending on the backend port and the router prefix).
    -   The `APIClient` constructor reads `ENDPOINT` using `os.getenv('ENDPOINT')` and validates its presence and format.
    -   Logging in the Streamlit app is also configured (file and stream handlers).
-   **`.env` Files**: A single `.env` file at the project root is typically used to store all necessary environment variables for both backend and frontend during local development. This file is gitignored.
    -   Example backend variables: `GEMINI_API_KEY`, `PINECONE_API_KEY`, `PINECONE_INDEX_NAME`, `DEBUG`, `PORT`.
    -   Example frontend variable: `ENDPOINT`.
-   **Validation**:
    -   Backend: `Config.validate_required_env_vars()` ensures critical service keys are present before the agent fully initializes. FastAPI/Pydantic handle request data validation.
    -   Frontend: `APIClient` validates the `ENDPOINT` environment variable.
-   **Access**:
    -   Backend: Configuration is accessed via the `Config` instance (e.g., `config.GEMINI_API_KEY` within components that have access to it, often through dependency injection via `request.app.state.config`).
    -   Frontend: `os.getenv('ENDPOINT')` is used in `streamlit_app.py`'s `APIClient`.
